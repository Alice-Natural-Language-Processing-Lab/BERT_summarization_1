{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_summarization_Apr19_2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOw6e/+lSJb8rJO9f2SuQOA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VincentK1991/BERT_summarization_1/blob/master/Primer_to_GPT2_abstractive_summarization_Apr19_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHo7WRJrk0Tb",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Abstractive summarization is a task of shortening a text by paraphrasing. Unlike extractive summarization where a \"key\" sentence is extracted from a paragraph, abstractive summarization relies on language modeling task, i.e.  predict the next workds/sentences.\n",
        "\n",
        "Re-framing the question is way, our task becomes how to use languge model to generate a summary given the main text.\n",
        "\n",
        "$TEXT => summary$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmdGmo3N6eMH",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://www.topbots.com/wp-content/uploads/2019/04/language-model-comparison_web.jpg)\n",
        "\n",
        "We will use Open-AI GPT2 to do this task. Unlike BERT which are bidirectional encoder, GPT2 is auto-regressive. It means the output is fed back in as a new input, like the recurrent-neural network. The information flows to the right only. This has **advantage and drawback**. Comparison with BERT is [here](https://www.topbots.com/generalized-language-models-bert-openai-gpt2/).\n",
        "\n",
        "The advantage is that it is good at generating new words on its own. i.e. paraphrasing, conversing, or the like; whereas BERT is very good at classification or finding highlight in a text.\n",
        "\n",
        "The drawback is that feeding output back as input can generate a vicious loop or getting off topic. For example, sometimes the output keeps repeating itself (i.e. the output feeding back to the model creating a recursive loop), or output slowly drifting to a new topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUVwnLqT6hi2",
        "colab_type": "text"
      },
      "source": [
        "GPT2 uses decoder stacks to do language model. Each stack is called \"transformer block\". The lite version of GPT2 that we use called \"distilgpt2\" has 6 blocks. The typical version (small) has 12 blocks.\n",
        "\n",
        "But first of it starts with embedding, so this is just to embed our tokenized words to high dimensional space. There are two kinds, \"WTE\" for \"word-token embedding\" and \"WPE\" for \"word-position embedding\". The \"WTE\" maps 50K vocab to 768 dimensions. The \"WPE\" maps word position up to 1024 in a text to 768 dimensions.\n",
        "\n",
        "            (wte): Embedding(50257, 768)\n",
        "            (wpe): Embedding(1024, 768)\n",
        "            (drop): Dropout(p=0.1, inplace=False)\n",
        "\n",
        "Each block has these stacks:\n",
        "\n",
        "          (0): Block(\n",
        "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "          (attn): Attention(\n",
        "            (c_attn): Conv1D()\n",
        "            (c_proj): Conv1D()\n",
        "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
        "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
        "          )\n",
        "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "          (mlp): MLP(\n",
        "            (c_fc): Conv1D()\n",
        "            (c_proj): Conv1D()\n",
        "            (dropout): Dropout(p=0.1, inplace=False)\n",
        "          )\n",
        "          )\n",
        "Basically, we have 6 of these blocks on top of one another.\n",
        "\n",
        "At the end, we have \n",
        "\n",
        "          (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True))\n",
        "          (lm_head): Linear(in_features=768, out_features=50257,\n",
        "           bias=False)\n",
        "\n",
        "Which maps the high dimensional space back to tokenized words.\n",
        "\n",
        "The key to success seems to be the use of clever attention mechanism. read more [here](http://jalammar.github.io/illustrated-gpt2/) or [here](https://github.com/harvardnlp/annotated-transformer/blob/master/The%20Annotated%20Transformer.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0JDr1i9AW5b",
        "colab_type": "text"
      },
      "source": [
        "The training of this kind of model works like most other neural network model. [Under the hood](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py), the model performs forward propagation, computes loss, performs backward propagation, performs step optimization, clip gradient norm, and repeat.\n",
        "\n",
        "The loss is called \"causal language modeling\" (CLM) loss, which is a cross entropy loss where the labels are input shifted to the right by 1 tokens. \n",
        "\n",
        "The exponentiated form of the loss is also called \"perplexity\", meaning how \"perplexed\" the model is to see the answer, not surprise = low score = good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHjC9f-wposn",
        "colab_type": "text"
      },
      "source": [
        "# set up packages\n",
        "\n",
        "This work will be pytorch-based. And use adaptation developed by [Huggingface](https://huggingface.co/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWmZ7EpBpfhJ",
        "colab_type": "code",
        "outputId": "e245b5a8-dc74-4b19-81a2-ee3a3190cdb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkUrPNEUqhzu",
        "colab_type": "code",
        "outputId": "87d216cf-84e0-40d5-e060-b45d3661f4e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "\n",
        "import os\n",
        "os.chdir('/content/transformers')\n",
        "\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "\n",
        "os.chdir('/content/transformers/examples')\n",
        "\n",
        "!pip install dict_to_obj"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 24459 (delta 3), reused 7 (delta 0), pack-reused 24449\u001b[K\n",
            "Receiving objects: 100% (24459/24459), 14.48 MiB | 26.05 MiB/s, done.\n",
            "Resolving deltas: 100% (17296/17296), done.\n",
            "Processing /content/transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.2)\n",
            "Collecting tokenizers==0.7.0rc7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/67/bcc06fc10f702b31ca6e7357775e5ba7b87e14cf276cd51940ddbe7354b6/tokenizers-0.7.0rc7-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.12.39)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 42.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.15.39)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers==2.8.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers==2.8.0) (2.8.1)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.8.0-cp36-none-any.whl size=570540 sha256=8bc29d24767393c1d27409ea3eafa46cb061085b800ce156307a856281c5ea9d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-reoocvhc/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=ef5bca4f0ea56db74d1c97bbc8de414f0eaf251c661e2ac08c2059dbe28edfc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.7.0rc7 transformers-2.8.0\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 2)) (2.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 3)) (0.22.2.post1)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 5)) (5.4.8)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/27/e9c95f45fc11f9093000d234564823aab762f517458f1aa1ad01ba51d5f2/sacrebleu-1.4.7-py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.8MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/6d/2b9a64cba1e4e6ecd4effbf6834b2592b54dc813654f84029758e5daeeb5/rouge_score-0.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 8)) (2.1.0)\n",
            "Collecting pytorch-lightning==0.7.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/53/0549dd9c44c90e96d217592e094e9c53ef39ae2fed0c5cdb7e57aca65af6/pytorch_lightning-0.7.3-py3-none-any.whl (203kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.18.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.28.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (46.1.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (2.21.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.6.0.post3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->-r ./examples/requirements.txt (line 4)) (2.3.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1MB 203kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (3.6.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score->-r ./examples/requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.12.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (19.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.3.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (4.38.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.21.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (2.3)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.3->-r ./examples/requirements.txt (line 9)) (1.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.51.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.8)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=25932c3fe474b046450349bcfd2feda9c89a7de96a13776b2bb56560cb6f29e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "\u001b[31mERROR: pytorch-lightning 0.7.3 has requirement future>=0.17.1, but you'll have future 0.16.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pytorch-lightning 0.7.3 has requirement tqdm>=4.41.0, but you'll have tqdm 4.38.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboardX, seqeval, portalocker, mecab-python3, sacrebleu, rouge-score, pytorch-lightning\n",
            "Successfully installed mecab-python3-0.996.5 portalocker-1.7.0 pytorch-lightning-0.7.3 rouge-score-0.0.3 sacrebleu-1.4.7 seqeval-0.0.12 tensorboardX-2.0\n",
            "Collecting dict_to_obj\n",
            "  Downloading https://files.pythonhosted.org/packages/71/84/95cb71e10a1627076f6e2ba6cd81683e4ba344a885aa2d8b38a9a33f53c4/dict_to_obj-0.0.2.tar.gz\n",
            "Building wheels for collected packages: dict-to-obj\n",
            "  Building wheel for dict-to-obj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dict-to-obj: filename=dict_to_obj-0.0.2-cp36-none-any.whl size=1444 sha256=65fc87f213d696aea0c10d90907d9fcebc9c86e3d6967a80f19367b8837af082\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/71/a8/727ff01010936e40001af4c79983f505aecb5f271faa91c3e3\n",
            "Successfully built dict-to-obj\n",
            "Installing collected packages: dict-to-obj\n",
            "Successfully installed dict-to-obj-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izSZ1e7drQvQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "52e11418-9547-4960-dae6-2b4829d5545e"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7in4-STgrTaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import run_language_modeling  # package from huggingface\n",
        "import run_generation # package from huggingface\n",
        "from dict_to_obj import DictToObj\n",
        "import collections\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hp8SgrFuFNb",
        "colab_type": "code",
        "outputId": "c4f083bb-0b86-45be-cae5-bee443499516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2eFXGXfkaOh",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tuning the model\n",
        "\n",
        "I use 100K samples (about 300MB of txt file) to train on GPU. I planned to run for ~ 5 epochs, but one epoch already takes about 2 hours. So I just run 1 epoch.\n",
        "\n",
        "I use 100K samples for evaluation, which is also overkilled. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVCkALrKrVf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --output_dir='/content/drive/My Drive/finetuned_models/Newsroom' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=distilgpt2 \\\n",
        "    --save_total_limit=5 \\\n",
        "    --num_train_epochs=7.0 \\\n",
        "    --do_train \\\n",
        "    --overwrite_output_dir \\\n",
        "    --evaluate_during_training \\\n",
        "    --logging_steps=5000 \\\n",
        "    --save_steps=5000 \\\n",
        "    --train_data_file='/content/drive/My Drive/Colab Notebooks/GPT-2/Newsroom/trainFile_small_format_Apr19_2020.txt' \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file='/content/drive/My Drive/Colab Notebooks/GPT-2/Newsroom/devFile_format_Apr19_2020.txt' \\\n",
        "    --per_gpu_train_batch_size=256 \\\n",
        "    --per_gpu_eval_batch_size=256 \\\n",
        "    --block_size=5 \\\n",
        "    --gradient_accumulation_steps=5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ptM-VJqQVR",
        "colab_type": "code",
        "outputId": "8d74f43c-75bd-4ff3-8992-0800af6f27ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!ls '/content/drive/My Drive/finetuned_models/Newsroom'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint-10000  checkpoint-25000    test_eval_checkpoint-5000\n",
            "checkpoint-15000  checkpoint-5000\n",
            "checkpoint-20000  eval_results_1.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ole7QuFkmDD",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate the Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XliRkN3m0Jft",
        "colab_type": "text"
      },
      "source": [
        "run the evaluation using run_language_modeling command line directly. We will use the weights and model configuration saved in the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOkvE-dD19Cz",
        "colab_type": "code",
        "outputId": "72ab92a2-3029-4b06-f160-eba10779b85e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --train_data_file='/content/drive/My Drive/Colab Notebooks/GPT-2/Newsroom/trainFile_small_format_Apr19_2020.txt' \\\n",
        "    --output_dir='/content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path='/content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000' \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file='/content/drive/My Drive/Colab Notebooks/GPT-2/Newsroom/devFile_small2_format_Apr19_2020.txt' \\\n",
        "    --per_gpu_eval_batch_size=2 \\\n",
        "    --block_size=128 \\"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/20/2020 14:53:02 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/20/2020 14:53:03 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000/config.json\n",
            "04/20/2020 14:53:03 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "04/20/2020 14:53:03 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000/config.json\n",
            "04/20/2020 14:53:03 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "04/20/2020 14:53:03 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/20/2020 14:53:03 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000/added_tokens.json. We won't load it.\n",
            "04/20/2020 14:53:03 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000/vocab.json\n",
            "04/20/2020 14:53:03 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000/merges.txt\n",
            "04/20/2020 14:53:03 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/20/2020 14:53:03 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000/special_tokens_map.json\n",
            "04/20/2020 14:53:03 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000/tokenizer_config.json\n",
            "04/20/2020 14:53:06 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000/pytorch_model.bin\n",
            "04/20/2020 14:53:15 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=128, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=False, eval_all_checkpoints=False, eval_data_file='/content/drive/My Drive/Colab Notebooks/GPT-2/Newsroom/devFile_small2_format_Apr19_2020.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='/content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=2, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='/content/drive/My Drive/Colab Notebooks/GPT-2/Newsroom/trainFile_small_format_Apr19_2020.txt', warmup_steps=0, weight_decay=0.0)\n",
            "04/20/2020 14:53:15 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000']\n",
            "04/20/2020 14:53:15 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000/config.json\n",
            "04/20/2020 14:53:15 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "04/20/2020 14:53:15 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-15000/pytorch_model.bin\n",
            "04/20/2020 14:53:18 - INFO - __main__ -   Loading features from cached file /content/drive/My Drive/Colab Notebooks/GPT-2/Newsroom/gpt2_cached_lm_128_devFile_small2_format_Apr19_2020.txt\n",
            "04/20/2020 14:53:18 - INFO - __main__ -   ***** Running evaluation checkpoint-15000 *****\n",
            "04/20/2020 14:53:18 - INFO - __main__ -     Num examples = 31931\n",
            "04/20/2020 14:53:18 - INFO - __main__ -     Batch size = 2\n",
            "Evaluating: 100% 15966/15966 [02:24<00:00, 110.74it/s]\n",
            "04/20/2020 14:55:43 - INFO - __main__ -   ***** Eval results checkpoint-15000 *****\n",
            "04/20/2020 14:55:43 - INFO - __main__ -     perplexity = tensor(65.1425)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtoSZ0osJsa1",
        "colab_type": "text"
      },
      "source": [
        "# generate text\n",
        "\n",
        "To do this, we will call a model and do a forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvojDIxPKDiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_models/Newsroom/checkpoint-25000/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb4epoBDONrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PROMPT = \"\"\"<|startoftext|> the precolonial era, the area of present-day New York City was inhabited by Algonquian Native Americans, including the Lenape. Their homeland, known as Lenapehoking, included Staten Island, Manhattan, the Bronx, the western portion of Long Island (including the areas that would later become the boroughs of Brooklyn and Queens), and the Lower Hudson Valley.\n",
        "The first documented visit into New York Harbor by a European was in 1524 by Giovanni da Verrazzano, a Florentine explorer in the service of the French crown. He claimed the area for France and named it Nouvelle Angoulême (New Angoulême). A Spanish expedition, led by the Portuguese captain Estêvão Gomes sailing for Emperor Charles V, arrived in New York Harbor in January 1525 and charted the mouth of the Hudson River, which he named Río de San Antonio (Saint Anthony's River). The Padrón Real of 1527, the first scientific map to show the East Coast of North America continuously, was informed by Gomes' expedition and labeled the northeastern United States as Tierra de Esteban Gómez in his honor. <|summarize|>\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RRkEM16Jxy_",
        "colab_type": "code",
        "outputId": "9432a96c-08ad-493c-c4d0-715feabbad80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        }
      },
      "source": [
        "tokenizer = run_language_modeling.AutoTokenizer.from_pretrained(CHECKPOINT_PATH)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/20/2020 18:15:23 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-25000/config.json\n",
            "04/20/2020 18:15:23 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "04/20/2020 18:15:23 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/finetuned_models/Newsroom/checkpoint-25000/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/finetuned_models/Newsroom/checkpoint-25000/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/20/2020 18:15:23 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-25000/added_tokens.json. We won't load it.\n",
            "04/20/2020 18:15:23 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-25000/vocab.json\n",
            "04/20/2020 18:15:23 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-25000/merges.txt\n",
            "04/20/2020 18:15:23 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/20/2020 18:15:23 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-25000/special_tokens_map.json\n",
            "04/20/2020 18:15:23 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-25000/tokenizer_config.json\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biIoY5ZPyzUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjYwQVafKGhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_prompt = tokenizer.encode(PROMPT, add_special_tokens=False, return_tensors=\"pt\")\n",
        "encoded_prompt = encoded_prompt.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8KI_9ALKnDC",
        "colab_type": "code",
        "outputId": "f623c01b-60d4-4672-ae38-30a83b2bc682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "model = run_language_modeling.AutoModelWithLMHead.from_pretrained(CHECKPOINT_PATH)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/20/2020 18:15:30 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-25000/config.json\n",
            "04/20/2020 18:15:30 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "04/20/2020 18:15:30 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/finetuned_models/Newsroom/checkpoint-25000/pytorch_model.bin\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yypCpy2MXJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdSVR0HgLU58",
        "colab_type": "code",
        "outputId": "93f00901-c957-4ee7-b98f-9df55ba73ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "output_sequences = model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=200 + len(encoded_prompt[0]),\n",
        "      temperature=0.5,\n",
        "      decoder_start_token_id= '<|summarize|>',\n",
        "      top_k=50,\n",
        "      top_p=1.0,\n",
        "      repetition_penalty=None,\n",
        "      do_sample=True,\n",
        "      num_return_sequences=3)\n",
        " # Remove the batch dimension when returning multiple sequences\n",
        "if len(output_sequences.shape) > 2:\n",
        "  output_sequences.squeeze_()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/20/2020 19:47:33 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zllX9tRMEgH",
        "colab_type": "code",
        "outputId": "5c6ebe50-30e4-4562-942f-848925386584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "stop_token = '<|endoftext|>'\n",
        "generated_sequences = []  \n",
        "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "  generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "  # Decode text\n",
        "  text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "  # Remove all text after the stop token\n",
        "  text = text[: text.find(stop_token) if stop_token else None]\n",
        "\n",
        "  # Remove the excess text that was used for pre-processing\n",
        "  text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "\n",
        "  # Add the prompt at the beginning of the sequence.\n",
        "  total_sequence = PROMPT + text\n",
        "\n",
        "  generated_sequences.append(total_sequence)\n",
        "generated_sequences"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|startoftext|> the precolonial era, the area of present-day New York City was inhabited by Algonquian Native Americans, including the Lenape. Their homeland, known as Lenapehoking, included Staten Island, Manhattan, the Bronx, the western portion of Long Island (including the areas that would later become the boroughs of Brooklyn and Queens), and the Lower Hudson Valley.\\nThe first documented visit into New York Harbor by a European was in 1524 by Giovanni da Verrazzano, a Florentine explorer in the service of the French crown. He claimed the area for France and named it Nouvelle Angoulême (New Angoulême). A Spanish expedition, led by the Portuguese captain Estêvão Gomes sailing for Emperor Charles V, arrived in New York Harbor in January 1525 and charted the mouth of the Hudson River, which he named Río de San Antonio (Saint Anthony\\'s River). The Padrón Real of 1527, the first scientific map to show the East Coast of North America continuously, was informed by Gomes\\' expedition and labeled the northeastern United States as Tierra de Esteban Gómez in his honor. <|summarize|> and the S.Fernando, who was a little known for the first time, the late-day, and the New York, in the late 60\\'s, the \\'\\'The New York Stock Exchange, and the first and the second in the U.S.A.S. at the time of the arrival of the late 1940s, the Dutch and the first American colonies, the same-day, and the first time his father, the old-flier is the first American, with a \\'\\'Ceauf, the first lady, and the first-in-born physician who was born in the late 19th- and 19th century.\\n\\n\\n\\n\\n\\n\\n\\n\\n. \"The last year, the late-1940s and early-century, it was the first to be a major exhibition of the city of the year, the first to be called \\'\\'The New York, not the first to be the first to be published, the first i',\n",
              " '<|startoftext|> the precolonial era, the area of present-day New York City was inhabited by Algonquian Native Americans, including the Lenape. Their homeland, known as Lenapehoking, included Staten Island, Manhattan, the Bronx, the western portion of Long Island (including the areas that would later become the boroughs of Brooklyn and Queens), and the Lower Hudson Valley.\\nThe first documented visit into New York Harbor by a European was in 1524 by Giovanni da Verrazzano, a Florentine explorer in the service of the French crown. He claimed the area for France and named it Nouvelle Angoulême (New Angoulême). A Spanish expedition, led by the Portuguese captain Estêvão Gomes sailing for Emperor Charles V, arrived in New York Harbor in January 1525 and charted the mouth of the Hudson River, which he named Río de San Antonio (Saint Anthony\\'s River). The Padrón Real of 1527, the first scientific map to show the East Coast of North America continuously, was informed by Gomes\\' expedition and labeled the northeastern United States as Tierra de Esteban Gómez in his honor. <|summarize|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n, the first- \"The first-century Italian architect and his wife and his wife, who is now a resident of the most famous.\\n\\n\\n\\n\\n\\n\\'\\'The only one of the most famous and famous and the first-century architect, the first-in-century architect, who went to the late 17th-century art world, who was at the city. The architect of the University of Paris, and the firstborn artist, who was a child, in the late 19th-century German-style, is a painter, the first-century architect, was a great-old, and now-born architect, \\'\\'the \"a very famous painter of the most famous and beloved and widely admired by the late artist. He was a painter, in the early 19th-century Italian artist, and had his father, who was a great-grandson of the late-in-law, the late-wife o',\n",
              " \"<|startoftext|> the precolonial era, the area of present-day New York City was inhabited by Algonquian Native Americans, including the Lenape. Their homeland, known as Lenapehoking, included Staten Island, Manhattan, the Bronx, the western portion of Long Island (including the areas that would later become the boroughs of Brooklyn and Queens), and the Lower Hudson Valley.\\nThe first documented visit into New York Harbor by a European was in 1524 by Giovanni da Verrazzano, a Florentine explorer in the service of the French crown. He claimed the area for France and named it Nouvelle Angoulême (New Angoulême). A Spanish expedition, led by the Portuguese captain Estêvão Gomes sailing for Emperor Charles V, arrived in New York Harbor in January 1525 and charted the mouth of the Hudson River, which he named Río de San Antonio (Saint Anthony's River). The Padrón Real of 1527, the first scientific map to show the East Coast of North America continuously, was informed by Gomes' expedition and labeled the northeastern United States as Tierra de Esteban Gómez in his honor. <|summarize|> and her daughter,” by a group of the late artist, who went to the first-day in the late 18th-century art.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n, who, in the late 19th-year-century Italian-sixties, was born in the late 19th-century Italy, the first and was a woman whose first to beheaded by the late-sop, and the British artist Pauline de la B.Médame and his wife, a daughter, Jean-Mary, a French sculptor, and his wife, was a painter and the daughter of Jean-Louis XIV, who died in 1813, the son of the Italian painter, and his cousin, and his father-in-prouded daughter, who was one of the first-century art historian, and a poet, was a painter of the late-in-Louis XIV.\\n\\n\\n\\n\\n\\n\\n\\n, the first-i\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9sa5_4eqSRr",
        "colab_type": "text"
      },
      "source": [
        "## future direction\n",
        "\n",
        "Still at this stage, the summarization looks gibberish.\n",
        "\n",
        "There are many things to try.\n",
        "For example,\n",
        "- Try changing the block_size, the first trial I use block size = 5. This is very likely a wrong choice as it is too short. Perhaps try, longer like 512, 1024 or use default by not specifying or specify as -1.\n",
        "\n",
        "- size of the dev file (initially use 100K samples, this takes too long) 10K sample would have been fine I think. \n",
        "\n",
        "- and gradient accumulation. Perhaps, if I use bigger batch size, I don't have to use this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj8oYis6lEx8",
        "colab_type": "text"
      },
      "source": [
        "# Citation\n",
        "\n",
        "[distilgpt2](https://huggingface.co/distilgpt2)\n",
        "- a lite version of GPT2, making it more compact and faster to train\n",
        "\n",
        "[example code for training GPT2](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py)\n",
        "- An example by Huggingface on how to train GPT2. In this notebook, we simply call this command directly to train GPT2.\n",
        "\n",
        "[example code for language generation](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py)\n",
        "- Example by Huggingface on how to generate text from pre-trained or fine-tuned GPT2. \n",
        "\n",
        "[Dataset](https://summari.es/)\n",
        "- We use first 100K summary-text pairs in the training set to train the model.\n",
        "- This dataset also has articles of various length, making training more challenging (!?). Other dataset that I see has articles that are shorter and more uniform in length.\n",
        "\n",
        "\n",
        "[blog post on how to train GPT2](https://minimaxir.com/2019/09/howto-gpt2/)\n",
        "\n",
        "[Annoted notbook on how attention works](https://github.com/harvardnlp/annotated-transformer/blob/master/The%20Annotated%20Transformer.ipynb)\n",
        "\n",
        "[Blog post on attention](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
        "\n",
        "[illutration of how GPT2 works](http://jalammar.github.io/illustrated-gpt2/)\n",
        "- I use this as a guide to strategize how to train GPT2 for abstractive summarization.\n",
        "\n",
        "- Basically from what I understand, The training set just contains pairs of article texts and their summaries, separated by a keyword token. In this case, I use '<|startoftext|>' to denote where the article text starts. '<|summarize|> ' to denote where the text ends and the summary starts. ' <|endoftext|>' to denote where the summary ends. This part is what I'm not sure about what kind of tokens I should use.  \n",
        "\n",
        "- I did not specify to the model tokenizer that they are special tokens. Perhaps this would make a big difference if I did. So, this is something to play around with. "
      ]
    }
  ]
}